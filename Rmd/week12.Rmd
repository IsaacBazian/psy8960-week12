---
title: "PSY 8960 Week 12"
author: "Isaac Bazian"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Script Settings and Resources
```{r script_settings_and_resources, message = FALSE}
#setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) comment for now, probably won't need
library(tidyverse)
library(RedditExtractoR)
library(tm)
library(qdap)
library(textstem)
library(RWeka)
library(ldatuning)
library(topicmodels)
library(parallel)
library(doParallel)
library(tidytext)
library(wordcloud)
```

## Data Import and Cleaning

```{r data_import}
# riopsychology_urls <- find_thread_urls(keywords = NA, sort_by = "new", subreddit = "IOPsychology", period = "year")
# 
# riopsychology_content <- get_thread_content(riopsychology_urls$url)
# 
# title <- riopsychology_content$threads$title
# upvotes <- riopsychology_content$threads$upvotes
# 
# week12_data <- tibble(title, upvotes)
# 
# write_csv(week12_data, "../data/week12_data.csv")


week12_tbl <- read_csv("../data/week12_data.csv")


```




```{r preprocess}
week12_tbl_pre_preprocess <- week12_tbl %>% 
  mutate(across("title", str_replace_all, "-|/", " ")) #This is done because otherwise words connected by a hyphen or slash end up as one token

io_corpus_original <- VCorpus(VectorSource(week12_tbl_pre_preprocess$title))

io_corpus_with_empty <- io_corpus_original %>% 
  tm_map(content_transformer(replace_abbreviation)) %>% 
  tm_map(content_transformer(replace_contraction)) %>% 
  tm_map(content_transformer(replace_symbol)) %>% #Dollar symbol came up
  tm_map(content_transformer(str_to_lower)) %>% 
  #tm_map(content_transformer(replace_number)) %>% #When this was included, numbers that reflect the year dominated the first category
  tm_map(removePunctuation) %>% 
  tm_map(removeWords, c(stopwords("en"), "io", "i o", "io psychology", "i o psychology", "io psych", "i o psych", "riopsychology", "iopsychology", "psychologist", "psychologists")) %>% 
  tm_map(stripWhitespace) %>% 
  tm_map(content_transformer(lemmatize_words))

io_corpus <- io_corpus_with_empty %>% 
  tm_filter(FUN = function(x) { return(nchar(stripWhitespace(x$content)[[1]]) > 1) }) #RL's code, changed > 0 to > 1 because my code made empty entries " " rather than "", so 1 character rather than 0
```


```{r compare_them}
compare_them <- function(corp1, corp2) {
  rows <- 1:length(corp1)
  pick <- sample(rows, 1)
  corp_list <- list("Row" = pick, Original = content(corp1[[pick]]), "Preprocessed" = content(corp2[[pick]]))
  return(corp_list)
}

compare_them(io_corpus_original, io_corpus_with_empty) #We do this with io_corpus_with_empty rather than io_corpus because this way the filtered values aren't removed yet, so the indexes match up with the original. Otherwise if the random value is above where the first value has been dropped, you get two different documents.
```
## Analysis


```{r bigram_dtm}
myTokenizer <- function(x) { 
  NGramTokenizer(x, Weka_control(min=1, max=2)) 
  }
io_dtm <- DocumentTermMatrix(io_corpus, control = list(tokenize = myTokenizer))
io_slim_dtm <- removeSparseTerms(io_dtm, .996)

io_dtm #REMOVE LATER, CODING CHECK
io_slim_dtm

# io_dtm_tbl <- as_tibble(as.matrix(io_dtm))#TEMP TO TROUBLESHOOT
# io_dtm_tbl <- io_dtm_tbl %>% 
#   mutate(sums = rowSums(io_dtm_tbl)) %>% 
#   mutate(zero = (sums == 0)) %>% 
#   select(sums, zero) #487, 570, 737, 

```

Maybe put this in Visualization?
```{r lda_tuning}
local_cluster <- makeCluster(7)
registerDoParallel(local_cluster)

lda_tuning <- FindTopicsNumber(
  io_dtm,
  topics = seq(2, 20, 1),
  metrics = c("Griffiths2004",
              "CaoJuan2009",
              "Arun2010",
              "Deveaud2014"),
  verbose = T
  )

FindTopicsNumber_plot(lda_tuning)


stopCluster(local_cluster)
registerDoSEQ()
```

```{r lda_results}
lda_results <- LDA(io_dtm, 5)

lda_betas <- tidy(lda_results, matrix="beta")

# lda_betas %>%
# group_by(topic) %>%
# top_n(10, beta) %>%
# arrange(topic, -beta) %>%
# View


lda_gammas <- tidy(lda_results, matrix="gamma")

gamma_vals <- lda_gammas %>%
group_by(document) %>%
top_n(1, gamma) %>%
slice(1) %>%
ungroup %>%
mutate(document = as.numeric(document)) %>%
arrange(document)


week12_tbl_ids <- week12_tbl %>% 
  mutate(doc_id = as.character(1:nrow(week12_tbl)))

topics_tbl_ids <- tibble(doc_id = Docs(io_dtm))

topics_tbl <- topics_tbl_ids %>% 
  left_join(y = week12_tbl_ids) %>%
  select(-upvotes) %>% #Getting rid of upvotes here because assignment language
  # implies final_tbl should be made from contents of topic_tbl, so I will add
  # upvotes back in when making final_tbl later
  mutate(topic = gamma_vals$topic,
         probability = gamma_vals$gamma)
         

```


```{r upvotes_by_topic}

final_tbl <- topics_tbl %>% 
  left_join(week12_tbl_ids) %>% 
  mutate(topic = as.factor(topic))

summary(lm(upvotes ~ topic, data = final_tbl))
TukeyHSD(aov(upvotes ~ topic, data = final_tbl))

```


## Visualization

```{r wordcloud}
wordcloud_tbl <- as_tibble(as.matrix(io_dtm))

wordcloud(names(wordcloud_tbl), colSums(wordcloud_tbl), max.words = 15, colors = brewer.pal(9, "Oranges")) #15 seems necessary to fit on page
```


