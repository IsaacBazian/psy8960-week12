---
title: "PSY 8960 Week 12"
author: "Isaac Bazian"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Script Settings and Resources
```{r script_settings_and_resources, message = FALSE}
#setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) comment for now, probably won't need
library(tidyverse)
library(RedditExtractoR)
library(tm)
library(qdap)
library(textstem)
library(RWeka)
library(ldatuning)
library(topicmodels)
library(parallel)
library(doParallel)
library(tidytext)
library(wordcloud)
```

## Data Import and Cleaning

This code makes use of the RedditExtractoR package to get the past year's worth of posts from r/IOPsychology. we simply tell the find_thread_urls which subreddit we're interested in, tell it to sort by 'new' to have posts sorted by date, and tell it to pull all posts from the past year. We then get the titles and numbers of upvotes for each post, put that information in a tibble, and save that tibble as a CSV. We can now simply call that data rather than importing the posts all over again every time.
```{r data_import}
# riopsychology_urls <- find_thread_urls(keywords = NA, sort_by = "new", subreddit = "IOPsychology", period = "year")
# 
# riopsychology_content <- get_thread_content(riopsychology_urls$url)
# 
# title <- riopsychology_content$threads$title
# upvotes <- riopsychology_content$threads$upvotes
# 
# week12_data <- tibble(title, upvotes)
# 
# write_csv(week12_data, "../data/week12_data.csv")


week12_tbl <- read_csv("../data/week12_data.csv")


```



With the data in hand, we not move on to preprocessing. We first remove all hyphens and slashes, as otherwise two words connected by a '-' or '/' show up as one token later on. We then make the data into a corpus and apply a variety of preprocessing steps to impose more consistency on the data - we break abbreviations and contractions down into their component words, replace symbols with their words meanings (I'd notices the dollar sign, $, in a few posts and wanted it treated consistently). At this point, the documents in the corpora have as many words as they're going to, so we can start removing and changing words. We make all words lowercase for consistent treatment, remove punctuation, remove stopwords (doing this after making words lowercase and removing punctuation really cuts down on how many common variations there are of 'I-O Psychology'), strip the white space, and finally lemmatize the words to unify similar words into their meaningful cores. Finally, we remove empty entries, as some posts were purely made of conventional stopwords and I-O Psychology terms. I elected not to change numbers into words, because when I did so the numbers associated with the past year, 2022, dominated the first topic identified by my LDA later.
```{r preprocess}
week12_tbl_pre_preprocess <- week12_tbl %>% 
  mutate(across("title", str_replace_all, "-|/", " ")) #This is done because otherwise words connected by a hyphen or slash end up as one token

io_corpus_original <- VCorpus(VectorSource(week12_tbl_pre_preprocess$title))

io_corpus_with_empty <- io_corpus_original %>% 
  tm_map(content_transformer(replace_abbreviation)) %>% 
  tm_map(content_transformer(replace_contraction)) %>% 
  tm_map(content_transformer(replace_symbol)) %>% #Dollar symbol came up, so I included this
  tm_map(content_transformer(str_to_lower)) %>% 
  #tm_map(content_transformer(replace_number)) %>% #When this was included, numbers that reflect the year dominated the first category
  tm_map(removePunctuation) %>% 
  tm_map(removeWords, c(stopwords("en"), "io", "i o", "io psychology", "i o psychology", "io psych", "i o psych", "riopsychology", "iopsychology", "psychologist", "psychologists")) %>% 
  tm_map(stripWhitespace) %>% 
  tm_map(content_transformer(lemmatize_words))

io_corpus <- io_corpus_with_empty %>% 
  tm_filter(FUN = function(x) { return(nchar(stripWhitespace(x$content)[[1]]) > 1) }) #RL's code, changed > 0 to > 1 because my code made empty entries " " rather than "", so 1 character rather than 0
```

We write a function to compare the original corpus and our preprocessed corpus before we removed the empty rows (done so that the indexes matches up). Basically, the function picks a random number in the range of the corpora's length, gets the content of the document at that row index from each corpus, and displays the row number and contents. Running this function a number of times left me confident that my preprocessing was generally functioning as I expected it to.
```{r compare_them}
compare_them <- function(corp1, corp2) {
  rows <- 1:length(corp1)
  pick <- sample(rows, 1)
  corp_list <- list("Row" = pick, Original = content(corp1[[pick]]), "Preprocessed" = content(corp2[[pick]]))
  return(corp_list)
}

compare_them(io_corpus_original, io_corpus_with_empty) #We do this with io_corpus_with_empty rather than io_corpus because this way the filtered values aren't removed yet, so the indexes match up with the original. Otherwise if the random value is above where the first value has been dropped, you get two different documents.
```
## Analysis

We set up a tokenizer to grab unigrams and bigrams from the corpus. We then use this tokenizer to turn the corpus into a DTM, then also remove sparse terms from this DTM. Examining the outputs of these objects reveals that we have successfully brought the n/k ratio of the slim DTM into the desired range between 2/1 and 3/1 (in this case, 982/333).
```{r bigram_dtm}
myTokenizer <- function(x) { 
  NGramTokenizer(x, Weka_control(min=1, max=2)) 
  }
io_dtm <- DocumentTermMatrix(io_corpus, control = list(tokenize = myTokenizer))
io_slim_dtm <- removeSparseTerms(io_dtm, .996)

io_dtm
io_slim_dtm

```

We set up parallelization to find the number of appropriate topics for our data with LDA. The LDA graphs seem to change a bit each time I run this code, regardless of whether I run it parallel or serial and with our without a seed, but the general trend looking at the minimize metrics and where the maximize metrics cross over seems to suggest that 3, 4, or 5 may be the best choices for number of topics.
```{r lda_tuning}
local_cluster <- makeCluster(7)
registerDoParallel(local_cluster)

lda_tuning <- FindTopicsNumber(
  io_dtm,
  topics = seq(2, 20, 1),
  metrics = c("Griffiths2004",
              "CaoJuan2009",
              "Arun2010",
              "Deveaud2014"),
  verbose = T
  )

FindTopicsNumber_plot(lda_tuning)


stopCluster(local_cluster)
registerDoSEQ()
```

Based on the LDA tuning graphs, I examined the beta matrices with 3, 4, or 5 topics and tried to make sense of them. 3 topics seemed to yield the most interpretable topics, with 4 topics already starting to have difficult-to-conceptualize overlap between topics and 5 being even harder to parse. I therefore used 3 topics to get the beta and gamma matrices. I then made a tibble by combining the document IDs, the original post title of each document, the topic each document was assigned by the LDA, and the probability that the document belonged to that topic.
```{r lda_results}
lda_results <- LDA(io_dtm, 3)

lda_betas <- tidy(lda_results, matrix="beta")

lda_betas_top10 <- lda_betas %>%
group_by(topic) %>%
top_n(10, beta) %>%
arrange(topic, -beta)


lda_gammas <- tidy(lda_results, matrix="gamma")

gamma_vals <- lda_gammas %>%
group_by(document) %>%
top_n(1, gamma) %>%
slice(1) %>%
ungroup %>%
mutate(document = as.numeric(document)) %>%
arrange(document)


week12_tbl_ids <- week12_tbl %>% 
  mutate(doc_id = as.character(1:nrow(week12_tbl))) #This code adds IDs to the original documents, and when it's left joined the removed empty entries will not carry over

topics_tbl_ids <- tibble(doc_id = Docs(io_dtm))

topics_tbl <- topics_tbl_ids %>% 
  left_join(y = week12_tbl_ids, by = join_by(doc_id)) %>%
  select(-upvotes) %>% #Getting rid of upvotes here because assignment language
  # implies final_tbl should be made from contents of topic_tbl, so I will add
  # upvotes back in when making final_tbl later
  mutate(topic = gamma_vals$topic,
         probability = gamma_vals$gamma)
         

```

Question 1:

Question 2:


This code makes the final tibble by adding the upvote counts of each post, then running an ANOVA to determine if these topics differed substantially from each other in terms of how many upvotes posts belonging to each topic received. Results indicated that there was no significant difference between the topics in terms of how many upvotes they received.
```{r upvotes_by_topic}

final_tbl <- topics_tbl %>% 
  left_join(week12_tbl_ids, by = join_by(doc_id)) %>% 
  mutate(topic = as.factor(topic))

summary(aov(upvotes ~ topic, data = final_tbl))
#TukeyHSD(aov(upvotes ~ topic, data = final_tbl))

```


## Visualization

This code makes a wordcloud from the DTM, such that more common words are larger and a darker shade of orange. I have restricted it to the top 20 words with this particular scale because otherwise there are often problems where some words do not have space to be plotted. INTERPRETATION HERE
```{r wordcloud}
wordcloud_tbl <- as_tibble(as.matrix(io_dtm))

wordcloud(names(wordcloud_tbl), colSums(wordcloud_tbl), max.words = 20, colors = brewer.pal(9, "Oranges"), scale = c(3.5, 0.25))
```


