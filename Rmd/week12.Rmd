---
title: "PSY 8960 Week 12"
author: "Isaac Bazian"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Script Settings and Resources
```{r script_settings_and_resources}
#setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) comment for now, probably won't need
library(tidyverse)
library(jsonlite)
library(tm)
library(qdap)
library(textstem)
library(RWeka)
library(ldatuning)
library(topicmodels)
library(parallel)
library(doParallel)
library(tidytext)
library(wordcloud)

library(rvest) #Testing, use instead of jsonlite if it works
library(RedditExtractoR) #More testing, trying a specialty package
```

## Data Import and Cleaning

THIS IS NOT WHAT IS NEEDED ON THE FINAL ASSIGNMENT. This grabs 100 or so posts from the subreddit, not the past year. Will need to figure out how to do that and download the data later, but this is enough to get started on the rest of the code and I will just change how I get week12_tbl later.
```{r data_import_and_cleaning}
# riopsychology_list <- fromJSON("https://www.reddit.com/r/IOPsychology/.json?limit=100", flatten = TRUE)
# week12_tbl_original <- as_tibble(riopsychology_list$data$children)
# week12_tbl <- week12_tbl_original %>%
#   select(title = data.title,
#          upvotes = data.score)
```


```{r data_import_2}

# url = "https://old.reddit.com/r/IOPsychology/"
# hold_tbl <- tibble(title_loop = NA, upvotes_loop = NA)
# 
# 
# for (i in seq(25, 300, 25)){
# 
# riopsychology_html <- read_html(url)
# 
# title_loop <- riopsychology_html %>% 
#   html_elements(xpath = "//a[@data-event-action = 'title']") %>% 
#   html_text()
# 
# upvotes_loop <- riopsychology_html %>% 
#   html_elements(xpath = "//div[@class = 'score unvoted']") %>% 
#   html_text() %>% 
#   as.numeric() %>% 
#   replace_na(0)
# 
# loop_tbl <- tibble(title_loop, upvotes_loop)
# 
# hold_tbl <- bind_rows(hold_tbl, loop_tbl)
# 
# 
# 
# last_post_id <- html_elements(riopsychology_html, xpath = "//p/a") %>%
#   html_attr("href") %>% 
#   str_match("/r/IOPsychology/comments/(.......)/") %>% 
#   as_tibble() %>%
#   select(V2) %>% 
#   drop_na() %>% 
#   tail(1) %>% 
#   as.String()
#   
# 
# url <- paste0("https://old.reddit.com/r/IOPsychology/?count=", i, "&after=t3_", last_post_id)
# 
# print(i)
# Sys.sleep(2)
# 
# }
# 
# week12_tbl <- hold_tbl %>% 
#   drop_na() %>% 
#   rename(title = title_loop, upvotes = upvotes_loop)
# 
# #Seems to be screwing up somewhere around 200-250, getting duplicates
# 
# # week12_tbl %>%
# #   group_by_all() %>%
# #   filter(n()>1) %>%
# #   ungroup()
# 
# week12_dupes <- week12_tbl %>% 
#   add_count(title, upvotes) %>%
#   filter(n>1) %>%
#   distinct()
# 
# week12_tbl %>% 
#   distinct()

```

```{r data_import_3}
# riopsychology_urls <- find_thread_urls(keywords = NA, sort_by = "new", subreddit = "IOPsychology", period = "year")
# 
# riopsychology_content <- get_thread_content(riopsychology_urls$url)
# 
# title <- riopsychology_content$threads$title
# upvotes <- riopsychology_content$threads$upvotes
# 
# week12_data <- tibble(title, upvotes)
# 
# write_csv(week12_data, "../data/week12_data.csv")


week12_tbl <- read_csv("../data/week12_data.csv")


```




```{r preprocess}
week12_tbl_pre_preprocess <- week12_tbl %>% 
  mutate(across("title", str_replace_all, "-|/", " ")) #This is done because otherwise words connected by a hyphen or slash end up as one word with removePunctuation

io_corpus_original <- VCorpus(VectorSource(week12_tbl_pre_preprocess$title))
io_corpus <- io_corpus_original %>% 
  tm_map(content_transformer(replace_abbreviation)) %>% 
  tm_map(content_transformer(replace_contraction)) %>% 
  tm_map(content_transformer(replace_symbol)) %>% #Dollar symbol came up
  tm_map(content_transformer(str_to_lower)) %>% 
  tm_map(content_transformer(replace_number)) %>%
  tm_map(removePunctuation) %>% 
  tm_map(removeWords, c(stopwords("en"), "io", "i o", "io psychology", "i o psychology", "io psych", "i o psych", "riopsychology", "iopsychology")) %>% 
  tm_map(stripWhitespace) %>% 
  tm_map(content_transformer(lemmatize_strings))

content(io_corpus_original[[1]]) #REMOVE LATER, CODING CHECK
content(io_corpus[[1]])
```


```{r compare_them}
compare_them <- function(corp1, corp2) {
  rows <- 1:length(corp1)
  pick <- sample(rows, 1)
  corp_list <- list("Original" = content(corp1[[pick]]), "Preprocessed" = content(corp2[[pick]]))
  return(corp_list)
}

compare_them(io_corpus_original, io_corpus)
```
## Analysis

SPARSENESS SLIMMING WILL NEED TO CHANGE WITH NEW DATA
```{r bigram_dtm}
myTokenizer <- function(x) { 
  NGramTokenizer(x, Weka_control(min=1, max=2)) 
  }
io_dtm <- DocumentTermMatrix(io_corpus, control = list(tokenize = myTokenizer))
io_slim_dtm <- removeSparseTerms(io_dtm, .995)

io_dtm #REMOVE LATER, CODING CHECK
io_slim_dtm
```

Maybe put this in Visualization?
```{r lda_tuning}
local_cluster <- makeCluster(7)
registerDoParallel(local_cluster)

lda_tuning <- FindTopicsNumber(
  io_dtm,
  topics = seq(2, 10, 1),
  metrics = c("Griffiths2004",
              "CaoJuan2009",
              "Arun2010",
              "Deveaud2014"),
  verbose = T
  )

FindTopicsNumber_plot(lda_tuning)


stopCluster(local_cluster)
registerDoSEQ()
```

```{r lda_results}
lda_results <- LDA(io_dtm, 5)

lda_betas <- tidy(lda_results, matrix="beta")

# lda_betas %>%
# group_by(topic) %>%
# top_n(10, beta) %>%
# arrange(topic, -beta) %>%
# View


lda_gammas <- tidy(lda_results, matrix="gamma")

gamma_vals <- lda_gammas %>%
group_by(document) %>%
top_n(1, gamma) %>%
slice(1) %>%
ungroup %>%
mutate(document = as.numeric(document)) %>%
arrange(document)


topics_tbl <- tibble(doc_id = 1:length(io_corpus),
                     original = week12_tbl$title,
                     topic = gamma_vals$topic,
                     probability = gamma_vals$gamma)

```


```{r upvotes_by_topic}

final_tbl <- topics_tbl %>% 
  mutate(upvotes = week12_tbl$upvotes,
         topic = as.factor(topic))

TukeyHSD(aov(upvotes ~ topic, data = final_tbl))

```


## Visualization

```{r wordcloud}
wordcloud_tbl <- as_tibble(as.matrix(io_dtm))

wordcloud(names(wordcloud_tbl), colSums(wordcloud_tbl), max.words = 20, colors = brewer.pal(10, "Oranges"))
```


